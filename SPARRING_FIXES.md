# Project Titan ‚Äî Sparring Fixes Kontextsheet
## CTO-Analyse: Was ausgebessert werden muss

**Erstellt**: 2026-02-12
**Quelle**: Vergleich Titan (aktuell) vs. Ares/Maschina (alt) + CTO-Sparring (Block 1-17)
**Status**: ‚úÖ ALLE PHASEN ABGESCHLOSSEN (Phase 1-8)
**Fixes**: 28 dokumentiert ‚Äî 27 implementiert ‚úÖ | 1 bereits korrekt (Block 10 GMS-Ausschluss)
**Sparring**: ABGESCHLOSSEN ‚Äî Alle 17 Bl√∂cke analysiert, alle Fragen beantwortet

---

## PRIORIT√ÑT: KRITISCH

### FIX-1: ByteDance Deep-Search in Deep Sanitize
**Problem**: TikTok erkennt den User nach Genesis-Flow wieder ‚Äî verh√§lt sich nicht wie Fresh Install.
**Ursache**: Titan l√∂scht nur statische Pfade. ByteDance/TikTok legt versteckte Tracking-Verzeichnisse an mehreren Orten ab, die `pm clear` und statische `rm -rf` nicht erfassen.

**Fehlende Pfade/Pattern**:
```
/sdcard/.com.ss.android*          ‚Äî ByteDance Cross-App SDK
/sdcard/Documents/com.zhiliaoapp* ‚Äî TikTok Document-Tracking
/sdcard/Download/.log/            ‚Äî Versteckte Logs
/sdcard/.msync/                   ‚Äî ByteDance Cross-App Sync
/sdcard/Documents/.tmlog/         ‚Äî Versteckte TikTok-Logs
/sdcard/DCIM/.thumbnails/         ‚Äî TikTok-Metadata in Thumbnails
```

**Fehlende Aktionen** (aus Ares `deep_sanitize()` + CTO-Analyse Block 1):
1. `find /sdcard -name '.tt*' -o -name '*.tt*'` ‚Äî Alle versteckten TT-Dateien finden und l√∂schen
2. `find /sdcard -type d -name '*zhiliaoapp*'` ‚Äî Alle zhiliaoapp-Verzeichnisse (au√üerhalb Android/data)
3. `find /sdcard -type d -name '*com.ss.android*'` ‚Äî ByteDance SDK-Reste
4. `find /sdcard -type d -name '.msync'` ‚Äî ByteDance Cross-App Sync Verzeichnisse
5. Explizites L√∂schen der `BYTEDANCE_PATTERNS`:
   - `/sdcard/.com.ss.android*`
   - `/sdcard/Documents/com.zhiliaoapp*`
   - `/sdcard/.tt*`
   - `/sdcard/.msync/`
   - `/sdcard/Documents/.tmlog/`
   - `/sdcard/Download/.log/`
   - `/sdcard/Android/data/com.zhiliaoapp.musically/.tt*`

**Wo**: `host/engine/shifter.py` ‚Üí `deep_clean()` ‚Äî nach Schritt 4 (Tracking-Globs) einf√ºgen.

**Referenz**: Ares `core/shifter.py` Zeilen 1066-1108

---

### FIX-2: Cache-Verzeichnisse explizit pr√ºfen und l√∂schen
**Problem**: `pm clear` l√∂scht `/data/data/<pkg>/` aber nicht alle Cache-Pfade zuverl√§ssig. Manche Cache-Verzeichnisse werden von Android/System nach `pm clear` automatisch neu erstellt und enthalten Reste.

**Fehlende Cache-Pfade**:
```
/data/data/com.zhiliaoapp.musically/cache
/data/data/com.zhiliaoapp.musically/code_cache
/storage/emulated/0/Android/data/com.zhiliaoapp.musically/cache
```

**Aktion**: Nach `pm clear` explizit pr√ºfen ob diese Pfade existieren und nochmal `rm -rf` mit Root.

**Wo**: `host/engine/shifter.py` ‚Üí `deep_clean()` ‚Äî nach den ByteDance-Pattern-Suchl√§ufen.

**Referenz**: Ares `core/shifter.py` Zeilen 1110-1124

---

## PRIORIT√ÑT: HOCH

### FIX-3: Backup-Whitelist (nur Login-relevante Ordner)
**Problem**: Titans `backup()` und `backup_tiktok_dual()` erstellen ein volles tar von `/data/data/<pkg>/`. Das inkludiert auch Cache, Crash-Reports und andere Daten die beim Restore Probleme verursachen k√∂nnen (veraltete Caches, korrupte Temp-Dateien).

**Empfohlene √Ñnderung**: Statt `tar -cf - -C / data/data/com.zhiliaoapp.musically` nur die relevanten Unterordner sichern:
```bash
su -c 'tar -C /data/data/com.zhiliaoapp.musically -cf - shared_prefs databases files 2>/dev/null'
```

**Warum Whitelist besser ist**:
- `shared_prefs/` = Login-Session, Cookies, User-Preferences
- `databases/` = SQLite-DBs mit Account-Daten
- `files/` = Token-Dateien, Konfiguration
- `cache/`, `code_cache/`, `no_backup/` = Nicht n√∂tig, kann Probleme verursachen

**Wo**: `host/engine/shifter.py` ‚Üí `backup_tiktok_dual()` ‚Üí Pfad A tar-Befehl anpassen.

**Referenz**: Ares `core/shifter.py` Zeilen 1506-1512

---

### FIX-4: Integrity Guard (Dateianzahl + Gr√∂√üenvergleich) ‚úÖ IMPLEMENTIERT
**Problem**: Titan pr√ºft nur ob tar > 0 Bytes ist. Das erkennt keine teilweise korrupten Backups (z.B. wenn ADB-Verbindung w√§hrend Stream abbricht und nur 10% der Daten √ºbertragen wurden).

**Empfohlene √Ñnderung**: Nach Backup die Statistiken auf dem Ger√§t vs. lokal vergleichen:
1. Device: `find <path> -type f | wc -l` + `du -sb <path>`
2. Lokal: tar inspizieren oder entpacken + vergleichen
3. Toleranz: 5% Dateianzahl, 10% Gr√∂√üe (Dateisystem-Unterschiede)

**Wo**: `host/engine/shifter.py` ‚Üí `backup_tiktok_dual()` ‚Äî nach dem tar-Stream als Validierung.

**Referenz**: Ares `core/shifter.py` Zeilen 1159-1300

---

### FIX-5: CE-Storage Unlock-Check via `dumpsys window` ‚úÖ IMPLEMENTIERT
**Problem**: Titans `_check_ce_storage()` pr√ºft nur ob `/data/data/com.google.android.gms/shared_prefs` existiert. Das ist ein schwacher Proxy. Ares hat eine robustere Methode die den tats√§chlichen Lock-Screen-State pr√ºft.

**Empfohlene √Ñnderung**: Zus√§tzlich `dumpsys window` pr√ºfen:
```bash
# Keyguard im Fokus = gesperrt
dumpsys window windows | grep -i mCurrentFocus
# ‚Üí "Keyguard" oder "LockScreen" = gesperrt
# ‚Üí "Launcher" oder "Activity" = entsperrt
```

**Wo**: `host/engine/shifter.py` ‚Üí `_check_ce_storage()` erweitern.

**Referenz**: Ares `core/shifter.py` Zeilen 759-790

---

## PRIORIT√ÑT: MITTEL

### FIX-6: USB-Reconnect Simulation nach Reboot ‚úÖ IMPLEMENTIERT
**Problem**: Nach Reboot bleibt ADB manchmal in einem "Zombie-State" h√§ngen ‚Äî der Daemon meldet "device" aber Shell-Befehle scheitern. Ein USB-Modus-Toggle l√∂st das.

**Empfohlene √Ñnderung**: In `host/adb/client.py` ‚Üí `ensure_connection()` als Fallback:
```bash
# USB-Modus auf "none" setzen (trennt Verbindung)
setprop sys.usb.config none
sleep 2
# USB-Modus auf "mtp,adb" setzen (verbindet neu)
setprop sys.usb.config mtp,adb
sleep 3
```

**Wann triggern**: Nur wenn normaler Reconnect (kill-server/start-server) nach 2 Versuchen fehlschl√§gt.

**Referenz**: Ares `core/shifter.py` Zeilen 189-226

---

### FIX-7: `wm dismiss-keyguard` als Unlock-Fallback ‚úÖ IMPLEMENTIERT
**Problem**: Titans Unlock (Wakeup + Swipe) funktioniert meistens, aber nach Reboot kann der WindowManager tr√§ge sein und Swipes ignorieren. `wm dismiss-keyguard` umgeht das komplett.

**Empfohlene √Ñnderung**: Nach dem Swipe-Unlock als Fallback:
```bash
su -c 'wm dismiss-keyguard'
```

**Wo**: `host/adb/client.py` ‚Üí `unlock_device()` ‚Äî nach dem Swipe als zus√§tzlichen Schritt.

**Referenz**: Ares `core/shifter.py` Zeilen 450-462

---

## SPARRING BLOCK 1 ‚Äî Sterilize Logik (zus√§tzliche Findings)

### FIX-13: `pm clear` durch `pm uninstall --user 0` + `pm install-existing` ersetzen
**Priorit√§t**: HOCH
**Problem**: `pm clear` l√∂scht zwar `/data/data/<pkg>/`, aber es beh√§lt:
- Die App selbst (APK + OAT-optimierte Dateien)
- Die Berechtigungen/Permission-Grants
- Den Android-internen Package-State (first-run Flag, Notification-Channels, etc.)

Das bedeutet: TikTok erkennt nach `pm clear`, dass die App **nicht zum ersten Mal** gestartet wird. Die ‚ÄûWelcome"-Screens, Onboarding-Flows und erste Setup-Dialoge k√∂nnen √ºbersprungen werden, was Anti-Fraud-Systemen auff√§llt.

**Empfohlene √Ñnderung**:
```bash
# Statt:
pm clear com.zhiliaoapp.musically

# Besser:
pm uninstall --user 0 com.zhiliaoapp.musically   # Deinstalliert f√ºr User 0
pm install-existing com.zhiliaoapp.musically       # Re-installiert aus System-Cache
```

**Vorteil**: Erzwingt einen echten "First Launch"-State ‚Äî App verh√§lt sich wie frisch installiert.

**Risiko**: `pm install-existing` funktioniert nur wenn die APK noch im System-Cache liegt (bei User-Apps immer der Fall). Bei System-Apps ohnehin kein Problem.

**Alternative** (falls `install-existing` Probleme macht):
```bash
pm uninstall com.zhiliaoapp.musically
pm install /data/app/<pkg-path>/base.apk
```
Erfordert aber das Merken des APK-Pfads vorher.

**Wo**: `host/engine/shifter.py` ‚Üí `deep_clean()` ‚Äî `pm clear` durch `pm uninstall --user 0` + `pm install-existing` ersetzen.

---

### FIX-14: TikTok Settings-ContentProvider Werte bereinigen ‚úÖ IMPLEMENTIERT
**Priorit√§t**: MITTEL
**Problem**: TikTok kann eigene Werte √ºber den Android `Settings`-ContentProvider schreiben (`Settings.Global` oder `Settings.Secure`). Diese Werte √ºberleben `pm clear` und sogar `pm uninstall`, weil sie nicht App-spezifisch sondern **System-global** gespeichert werden.

Beispiele f√ºr TikTok-Tracking via Settings:
- Custom Device-IDs die TikTok √ºber `Settings.Secure.putString()` persistiert
- ByteDance SDK Tracking-Tokens
- Install-Referrer oder Attribution-Daten

**Empfohlene Aktion**: Nach `pm clear` / `pm uninstall` alle verd√§chtigen `Settings`-Eintr√§ge l√∂schen:
```bash
# Alle Settings durchsuchen nach TikTok/ByteDance-Referenzen
settings list secure | grep -i 'tiktok\|bytedance\|musically\|tt_\|ss_android'
settings list global | grep -i 'tiktok\|bytedance\|musically\|tt_\|ss_android'

# Gefundene Eintr√§ge l√∂schen
settings delete secure <key>
settings delete global <key>
```

**Wo**: `host/engine/shifter.py` ‚Üí `deep_clean()` ‚Äî nach `pm clear` / `pm uninstall` als neuer Schritt.

**Hinweis**: Die genauen Keys m√ºssen einmal manuell ermittelt werden (TikTok installieren, starten, dann `settings list` pr√ºfen). Alternativ kann der Xposed Debug-Log-Mode (FIX-12) diese Keys aufdecken.

---

## SPARRING BLOCK 4 ‚Äî Genesis Flow Logik

### FIX-9: Bridge-Verifikation auf ALLE Pfade ausweiten (Post-Reboot) ‚úÖ IMPLEMENTIERT
**Priorit√§t**: MITTEL
**Problem**: Nach dem Reboot in Schritt 7 (Hard Reset) wird nur der prim√§re Bridge-Pfad verifiziert (`/data/adb/modules/titan_verifier/titan_identity`). Die weiteren Kopien (`/sdcard/`, App-Ordner) werden nicht gepr√ºft. Wenn eine Kopie fehlt oder korrupt ist, merkt der Flow das nicht.

**Aktuell verifizierte Pfade** (nur 1):
```
/data/adb/modules/titan_verifier/titan_identity   ‚Üê NUR DIESER
```

**Soll verifiziert werden** (alle 3 Hauptpfade):
```
/data/adb/modules/titan_verifier/titan_identity   ‚Üê Prim√§r (Zygisk liest hier)
/sdcard/.titan_identity                            ‚Üê Backup (LSPosed liest hier)
/data/data/com.titan.verifier/files/.titan_identity ‚Üê App-Kopie (Audit)
```

**Aktion**: `serial=` Grep auf allen 3 Pfaden nach dem Reboot. Bei Mismatch ‚Üí WARNING (nicht FAIL, da Prim√§rpfad reicht). Bei Prim√§rpfad-Mismatch ‚Üí FAIL wie bisher.

**Wo**: `host/flows/genesis.py` ‚Üí Schritt 7 (Hard Reset), Abschnitt "POST-REBOOT BRIDGE VERIFICATION" (Zeile 509-540).

---

### FIX-10: GMS Ready vereinfachen (Option A ‚Äî nur Connectivity-Check)
**Priorit√§t**: HOCH
**Problem**: Schritt 9 (GMS Ready) f√ºhrt Finsky Kill + MinuteMaid + GMS Kickstart durch. Das ist ein Relikt aus der alten Architektur (vor v4.0 GMS-Schutz), als GMS bei jedem Genesis-Flow gel√∂scht wurde. Seit v4.0 wird GMS NIE gel√∂scht ‚Äî die Trust-Chain bleibt intakt. Der Kickstart-Code verursacht unn√∂tige Wartezeiten und l√§sst den Flow manchmal h√§ngen.

**Aktueller Ablauf** (Schritt 9):
1. Konnektivit√§ts-Check ‚Üê BEHALTEN
2. Finsky Kill (`am force-stop com.android.vending`) ‚Üê ENTFERNEN
3. MinuteMaid GMS Repair ‚Üê ENTFERNEN
4. GMS Kickstart (Checkin triggern) ‚Üê ENTFERNEN
5. GSF-ID Logging ‚Üê BEHALTEN (reine Info)

**Neuer Ablauf** (Schritt 9):
1. Konnektivit√§ts-Check (IP bereits in Schritt 8 best√§tigt ‚Üí schnell)
2. GSF-ID Logging (informativ, kein Wait)

**Timing-Ersparnis**: ~`GMS_KICKSTART_SETTLE_SECONDS` (3s) + MinuteMaid-Wartezeit + potenzielle H√§nger.

**Wo**: `host/flows/genesis.py` ‚Üí Schritt 9 (GMS Ready), Zeilen 634-721.

---

### FIX-11: TikTok Backup-Logik ‚Äî Auto-Backup wenn `tiktok_username` gesetzt
**Priorit√§t**: HOCH
**Problem**: Die Auto-Backup-Entscheidung im Genesis- und Switch-Flow ber√ºcksichtigt nicht, ob der User f√ºr das aktive Profil √ºberhaupt einen TikTok-Account eingerichtet hat. Es wird entweder blind gesichert (Switch) oder nur per Checkbox gesteuert (Genesis). Das f√ºhrt zu leeren/unn√∂tigen Backups oder fehlenden Backups.

**Gew√ºnschte Logik (f√ºr BEIDE Flows ‚Äî Genesis + Switch)**:
```
VOR JEDEM FLOW:
1. Finde aktives Profil (profiles.status = 'active')

2. WENN aktives Profil gefunden:
   a) Lade tiktok_username aus DB f√ºr dieses Profil
   b) WENN tiktok_username gesetzt (NOT NULL, nicht leer):
      ‚Üí IMMER backup_tiktok_dual() ausf√ºhren
      ‚Üí Grund: "User hat TikTok-Account eingerichtet ‚Üí Daten sichern"
   c) WENN tiktok_username NICHT gesetzt:
      ‚Üí KEIN Backup (kein Account ‚Üí nichts zu sichern)

3. WENN KEIN aktives Profil:
   a) Pr√ºfe Checkbox (backup_before) in der WebUI
   b) WENN Checkbox gesetzt ‚Üí Versuche Backup (User Override)
   c) WENN Checkbox NICHT gesetzt ‚Üí √úberspringe
```

**DB-Feld f√ºr Pr√ºfung**: `profiles.tiktok_username` (TEXT, nullable)

**Betroffene Methoden**:
- `GenesisFlow._find_active_profile()` ‚Üí muss `tiktok_username` mit zur√ºckgeben
- `GenesisFlow.execute()` ‚Üí Schritt 2 (Auto-Backup) ‚Äî Logik erweitern
- `SwitchFlow._find_active_profile()` ‚Üí muss `tiktok_username` mit zur√ºckgeben
- `SwitchFlow.execute()` ‚Üí Schritt 2 (Auto-Backup) ‚Äî Logik erweitern

**Wo**:
- `host/flows/genesis.py` ‚Üí `_find_active_profile()` + Schritt 2
- `host/flows/switch.py` ‚Üí `_find_active_profile()` + Schritt 2

---

### FIX-12: Xposed Debug-Log-Mode (Hook-Monitoring f√ºr WebUI) ‚úÖ IMPLEMENTIERT
**Priorit√§t**: MITTEL
**Problem**: Aktuell gibt es keine M√∂glichkeit zu sehen, welche Hooks TikTok tats√§chlich trifft und was TikTok f√ºr Werte empf√§ngt. Die Titan Verifier App pr√ºft aus ihrer eigenen Perspektive, aber nicht aus TikToks Prozess heraus.

**L√∂sung**: Debug-Log-Mode im `TitanXposedModule.kt` einbauen, der pro gehooktem API-Call mitloggt:
```
[HOOK] TikTok ‚Üí TelephonyManager.getDeviceId()     ‚Üí Spoofed: 355543XXXXXXX
[HOOK] TikTok ‚Üí Settings.Secure.getString(android_id) ‚Üí Spoofed: a1b2c3d4...
[HOOK] TikTok ‚Üí Build.SERIAL                        ‚Üí Spoofed: ABC123DEF456
[HOOK] TikTok ‚Üí WifiInfo.getMacAddress()             ‚Üí Spoofed: F4:F5:D8:XX:XX:XX
‚úó TikTok ‚Üí getAccounts()                             ‚Üí NOT HOOKED (!)
```

**Steuerung**: √úber einen Flag in der Bridge-Datei (`debug_hooks=1/0`) oder √ºber eine Shared-Pref die vom Host gesetzt wird.

**WebUI-Integration**: Logs via `logcat --pid=<tiktok_pid> -s TitanHook` in den Live-Log-Stream der WebUI einspeisen.

**Nutzen**: Zeigt sofort welche APIs gehooked werden, welche Werte gespooft werden, und wo L√ºcken sind. Essenziell f√ºr Debugging wenn TikTok den User trotz Genesis-Flow wiedererkennt.

**Wo**:
- `app/src/main/java/.../TitanXposedModule.kt` ‚Üí Log-Wrapper um jeden Hook
- `host/api/dashboard.py` ‚Üí Optional: Logcat-Filter f√ºr Hook-Logs
- Bridge-Datei ‚Üí neues Feld `debug_hooks=0` (Default: aus)

---

## SPARRING BLOCK 5 ‚Äî Switch Flow Logik

### FIX-15: Sandbox-L√ºcke ‚Äî Full-State Restore ohne TikTok Sandbox
**Priorit√§t**: HOCH
**Problem**: Wenn der Switch Flow im Full-State-Modus l√§uft (`profile_name` angegeben), ruft Schritt 5 (`restore_full_state()`) GMS + Accounts + TikTok **App-Daten** her. Schritt 6 (Restore TikTok) wird dann SKIPPED mit "Bereits in Schritt 5 enthalten".

ABER: `restore_full_state()` restored NUR `/data/data/<pkg>/` (App-Daten). Die **TikTok Sandbox** (`/sdcard/Android/data/<pkg>/`) wird NICHT restored! Die Sandbox enth√§lt:
- ByteDance SDK Device-Fingerprints
- Download-Cache und Medien
- TikTok SDK-Konfiguration

Das bedeutet: **Bei jedem Switch gehen TikToks Sandbox-Daten verloren**, auch wenn ein Sandbox-Backup existiert. TikTok muss die Sandbox beim n√§chsten Start komplett neu aufbauen ‚Äî das kann als verd√§chtig erkannt werden.

**L√∂sung**: Schritt 6 darf bei Full-State NICHT √ºbersprungen werden. Stattdessen:
```
Schritt 5 (Restore State): GMS + Accounts (wie bisher)
Schritt 6 (Restore TikTok): IMMER Dual-Path Restore ausf√ºhren:
  ‚Üí Pfad A: App-Daten aus tiktok/ Unterordner
  ‚Üí Pfad B: Sandbox aus sandbox/ Unterordner
```
Falls `restore_full_state()` bereits TikTok App-Daten restored hat, soll Schritt 6 nur noch die Sandbox nachladen.

**Betroffener Code**:
```python
# switch.py Zeile 390-393 ‚Äî AKTUELL (falsch):
if use_full_state:
    step.status = FlowStepStatus.SKIPPED
    step.detail = "Bereits in Schritt 5 (Full-State) enthalten"

# SOLL: Sandbox-Restore immer ausf√ºhren
```

**Wo**: `host/flows/switch.py` ‚Üí Schritt 6 (Restore TikTok), Zeilen 386-438.

---

### FIX-16: Mini-Clean vor Switch-Restore (ByteDance-Reste bereinigen)
**Priorit√§t**: HOCH
**Problem**: Der Switch Flow macht kein `pm clear` und kein Deep Clean vor dem Restore. Er √ºberschreibt nur die Daten via tar. Aber: Wenn TikTok zwischen dem letzten Backup und dem Switch neue Tracking-Dateien auf `/sdcard/` geschrieben hat, bleiben diese Reste liegen. Sie k√∂nnten das **alte** Profil verraten.

**Beispiel-Szenario**:
1. Profil A ist aktiv, TikTok l√§uft ‚Üí schreibt `/sdcard/.tt_device_id_v2`
2. Switch zu Profil B ‚Üí TikTok App-Daten werden aus Backup B restored
3. ABER: `/sdcard/.tt_device_id_v2` enth√§lt noch die Device-ID von Profil A!
4. TikTok liest die alte Tracking-Datei ‚Üí Cross-Profile Correlation m√∂glich

**L√∂sung**: Vor dem Restore (nach Safety Kill, vor Inject) einen gezielten Mini-Clean durchf√ºhren:
```
Neuer Sub-Schritt in Schritt 3 oder als eigener Schritt:
  1. rm -f /sdcard/.tt* /sdcard/.tg* /sdcard/.tobid*
  2. rm -rf /sdcard/.msync/ /sdcard/.com.ss.android*
  3. rm -rf /sdcard/Documents/com.zhiliaoapp* /sdcard/Documents/.tmlog/
  4. rm -rf /sdcard/Download/.log/
  5. find /sdcard -name '.tt*' -delete (vollst√§ndiger Sweep)
```

Dies ist eine Teilmenge von FIX-1 (ByteDance Deep-Search), aber speziell f√ºr den Switch-Kontext ‚Äî ohne `pm clear`, nur `/sdcard/` Tracking-Reste.

**Wo**: `host/flows/switch.py` ‚Üí nach Schritt 3 (Safety Kill), vor Schritt 4 (Inject). Alternativ: Eigene Methode `TitanShifter.clean_tracking_remnants()` die sowohl von Genesis (deep_clean) als auch Switch aufgerufen wird.

---

## SPARRING BLOCK 7 ‚Äî Auditor L√ºcken

### FIX-17: Host-Side Auditor erweitern (Full + Quick Audit) ‚úÖ IMPLEMENTIERT
**Priorit√§t**: HOCH
**Status**: ‚úÖ Implementiert (Phase 3)
**Problem**: Der `TitanAuditor` pr√ºft aktuell nur **4 Dinge**: Bridge existiert, Bridge-Serial, Input-Devices, Bridge-MAC. Er pr√ºft NICHT ob die kritischsten Spoofing-Felder korrekt in der Bridge stehen:

| Feld | Status | Risiko wenn fehlerhaft |
|---|---|---|
| `serial` + `boot_serial` | ‚úÖ Gepr√ºft | Niedrig (wird gepr√ºft) |
| `wifi_mac` | ‚úÖ Gepr√ºft | Niedrig (wird gepr√ºft) |
| `imei1` | ‚ùå NICHT gepr√ºft | **HOCH** ‚Äî TikTok Hauptidentifikator |
| `imei2` | ‚ùå NICHT gepr√ºft | Mittel |
| `gsf_id` | ‚ùå NICHT gepr√ºft | **HOCH** ‚Äî Google Correlation |
| `android_id` | ‚ùå NICHT gepr√ºft | **HOCH** ‚Äî SSAID, von TikTok oft abgefragt |
| `widevine_id` | ‚ùå NICHT gepr√ºft | Mittel ‚Äî DRM Fingerprint |
| `imsi` | ‚ùå NICHT gepr√ºft | Mittel ‚Äî SIM-Fingerprint |
| `sim_serial` | ‚ùå NICHT gepr√ºft | Mittel |
| `build_fingerprint` | ‚ùå NICHT gepr√ºft | Mittel ‚Äî Build-Consistency |

Wenn die Bridge korrupt ist aber zuf√§llig den richtigen Serial + MAC hat, meldet der Auditor **100%** ‚Äî aber IMEI/GSF-ID/Android-ID k√∂nnten falsch sein.

**L√∂sung ‚Äî Full Audit (Genesis Schritt 11)**:
Alle Felder der Bridge gegen die erwartete Identity pr√ºfen:
```python
# Neue Checks hinzuf√ºgen:
result.checks.append(self._check_bridge_field(bridge, expected, "imei1"))
result.checks.append(self._check_bridge_field(bridge, expected, "imei2"))
result.checks.append(self._check_bridge_field(bridge, expected, "gsf_id"))
result.checks.append(self._check_bridge_field(bridge, expected, "android_id"))
result.checks.append(self._check_bridge_field(bridge, expected, "widevine_id"))
result.checks.append(self._check_bridge_field(bridge, expected, "imsi"))
result.checks.append(self._check_bridge_field(bridge, expected, "sim_serial"))
result.checks.append(self._check_bridge_field(bridge, expected, "build_fingerprint", critical=False))
```

**L√∂sung ‚Äî Quick Audit (Switch Schritt 9)**:
Erweitern von nur Serial auf die 5 wichtigsten Felder:
```python
# Statt nur serial:
fields_to_check = ["serial", "imei1", "gsf_id", "android_id", "wifi_mac"]
for field in fields_to_check:
    actual = bridge.get(field, "")
    expected_val = getattr(expected, field, "")
    if actual != expected_val:
        return False  # Mismatch
return True
```

**Wichtig ‚Äî App-Ebene**: Der Host-Side Auditor pr√ºft die Bridge-Datei (was die Hooks LESEN SOLLEN). Ob die Apps die gespooften Werte auch tats√§chlich EMPFANGEN, wird durch FIX-12 (Xposed Debug-Log-Mode) abgedeckt. Beides zusammen ergibt die vollst√§ndige Verifikation.

**Wo**: `host/engine/auditor.py` ‚Üí `audit_device()` + `quick_audit()`

---

## SPARRING BLOCK 8 ‚Äî Network & IP

### FIX-18: IP-Duplikat-Erkennung (IP-Datenbank mit Collision-Check) ‚úÖ IMPLEMENTIERT
**Priorit√§t**: HOCH
**Status**: ‚úÖ Implementiert (Phase 3)
**Problem**: Die IP-Rotation via Flugmodus-Cycle erzwingt eine neue Modem-Session. ABER: Der Carrier (O2) kann dieselbe IP erneut zuweisen (IP-Pool ist begrenzt, Lease-Zuordnung erfolgt server-seitig). Aktuell wird die IP in `ip_history` gespeichert, aber es gibt **keinen Check** ob diese IP bereits von einem ANDEREN Profil benutzt wurde.

**Risiko**: Wenn Profil A und Profil B dieselbe √∂ffentliche IP verwenden, kann TikTok (oder jeder Netzwerk-Analyst) die beiden Accounts korrelieren ‚Äî identische IP = wahrscheinlich dasselbe Ger√§t.

**L√∂sung ‚Äî IP-Collision-Detection**:
Nach jedem IP-Check (in Genesis Schritt 8 und Switch Schritt 8) gegen die `ip_history` Tabelle pr√ºfen:

```sql
-- Pr√ºfe ob diese IP jemals von einem ANDEREN Profil benutzt wurde
SELECT DISTINCT profile_id, identity_id, flow_type, created_at
FROM ip_history
WHERE public_ip = ?
  AND profile_id != ?        -- Nicht das aktuelle Profil
ORDER BY created_at DESC
LIMIT 5
```

**Verhalten bei Collision**:
```
1. IP war noch NIE benutzt      ‚Üí ‚úÖ OK, Flow fortsetzen
2. IP war von DIESEM Profil     ‚Üí ‚úÖ OK (gleiches Profil, erwartbar)
3. IP war von ANDEREM Profil    ‚Üí ‚ö†Ô∏è WARNING ins Log + WebUI:
   "IP {ip} wurde bereits von Profil '{name}' am {datum} benutzt!
    Cross-Profile Korrelation m√∂glich."
4. IP war von 3+ Profilen       ‚Üí üî¥ CRITICAL WARNING:
   "IP {ip} wurde von {n} verschiedenen Profilen benutzt!
    Empfehlung: Wartezeit vor n√§chstem Flow erh√∂hen."
```

**KEIN Flow-Abbruch** bei IP-Collision ‚Äî nur Warning. Grund: Wir k√∂nnen die IP nicht √§ndern (Carrier kontrolliert den Pool). Aber der User muss informiert werden.

**Zus√§tzliche Metriken f√ºr WebUI**:
- "Unique IPs": Anzahl verschiedener IPs die je benutzt wurden
- "IP Reuse Rate": Prozentsatz der Flows die eine bereits benutzte IP bekommen haben
- "Letzte IP-Collision": Wann war der letzte Vorfall

**ZUS√ÑTZLICH ENTDECKT**: Der Switch Flow speichert die IP aktuell **NICHT** in `ip_history` oder `identities`! Nur der Genesis Flow ruft `record_ip()` und `update_identity_network()` auf. Das bedeutet: Beim Switch wird die IP zwar ermittelt und geloggt, aber nie in die DB geschrieben. ‚Üí FIX-18 muss auch den Switch Flow patchen, damit IPs dort ebenfalls persistiert werden.

**Au√üerdem**: Die Indizes auf `ip_history` sind im Schema **auskommentiert**:
```sql
-- CREATE INDEX IF NOT EXISTS idx_ip_ip        ON ip_history(public_ip);
-- CREATE INDEX IF NOT EXISTS idx_ip_identity  ON ip_history(identity_id);
-- CREATE INDEX IF NOT EXISTS idx_ip_time      ON ip_history(detected_at DESC);
```
Diese m√ºssen f√ºr die Collision-Detection aktiv sein (sonst Full-Table-Scan bei jedem Check).

**Wo**:
- `host/engine/db_ops.py` ‚Üí Neue Funktion `check_ip_collision(ip, current_profile_id)`
- `host/flows/genesis.py` ‚Üí Schritt 8 (Network Init), nach IP-Check
- `host/flows/switch.py` ‚Üí Schritt 8 (Network Init), nach IP-Check **+ `record_ip()` + `update_identity_network()` hinzuf√ºgen**
- `host/database.py` ‚Üí Indizes auf `ip_history` aktivieren
- `host/frontend/templates/dashboard.html` ‚Üí IP-Status im Header (optional)

---

## SPARRING BLOCK 13 ‚Äî Error Handling & Resilience

### FIX-22: Genesis Rollback erweitern ‚Äî `corrupted` nach Inject bei sp√§terem Fehler ‚úÖ IMPLEMENTIERT
**Priorit√§t**: HOCH
**Status**: ‚úÖ Implementiert (Phase 4)
**Problem**: Wenn Genesis Schritt 6 (Inject) erfolgreich ist, aber ein sp√§terer Schritt fehlschl√§gt (Hard Reset, Network Init, Capture State), bleibt die Identity in der DB als `active` ‚Äî obwohl der Flow FAILED ist. Das Ger√§t ist in einem unbekannten Zustand.

**L√∂sung**: Auch nach erfolgreichem Inject die Identity als `corrupted` markieren wenn der Flow danach fehlschl√§gt. Zus√§tzlich eine **Info-Meldung in der WebUI** anzeigen:
```
"‚ö† Identity '{name}' als corrupted markiert ‚Äî Genesis Flow nach Inject abgebrochen 
(Schritt {step_name} fehlgeschlagen). Bitte neuen Genesis-Flow starten."
```

**Aktueller Rollback** (nur bei Inject-Fehler):
```python
if db_identity_id and not any(
    s.name == "Inject" and s.status == FlowStepStatus.SUCCESS
    for s in result.steps
):
    await self._update_identity_status(db_identity_id, IdentityStatus.CORRUPTED)
```

**Neuer Rollback** (auch nach Inject bei sp√§terem Fehler):
```python
# Bei JEDEM Flow-Fehler die Identity als corrupted markieren
if db_identity_id:
    await self._update_identity_status(db_identity_id, IdentityStatus.CORRUPTED)
    logger.warning("Identity %d als corrupted markiert (Flow nach Inject fehlgeschlagen)", db_identity_id)
```

**Wo**: `host/flows/genesis.py` ‚Üí `except ADBError` + `except Exception` Handler (Zeilen 891-927).

---

### FIX-23: Backup-Resilience ‚Äî Atomic Write + Retry bei ADB-Abbruch ‚úÖ IMPLEMENTIERT
**Priorit√§t**: HOCH
**Status**: ‚úÖ Implementiert (Phase 4)
**Problem**: Backup schreibt direkt in die finale Datei. Wenn ADB w√§hrend des tar-Streams die Verbindung verliert, bleibt eine korrupte Teildatei liegen. Beim n√§chsten Restore wird diese als g√ºltiges Backup behandelt.

**L√∂sung ‚Äî Dreistufig**:

**Stufe 1: Atomic Write**
```
1. Schreibe nach app_data.tar.tmp (nicht direkt in .tar)
2. Bei Erfolg: rename .tmp ‚Üí .tar
3. Bei Fehler: l√∂sche .tmp, altes .tar bleibt intakt
```

**Stufe 2: Retry bei ADB-Abbruch**
```
1. Fange ADBError/ADBTimeoutError beim tar-Stream
2. L√∂sche korrupte .tmp Datei
3. ADB-Verbindung wiederherstellen (ensure_connection)
4. Erneuter Versuch (max. 3 Retries mit exponential backoff)
5. Wenn alle Retries fehlschlagen ‚Üí Flow abbrechen
```

**Stufe 3: Info-Meldung bei Abbruch**
```
"üî¥ Backup fehlgeschlagen nach 3 Versuchen ‚Äî ADB-Verbindung instabil.
 Bestehendes Backup bleibt erhalten. Flow abgebrochen."
```
‚Üí WebUI zeigt die Meldung als Error-Notification an.

**Wichtig**: Altes Backup darf NIE mit korrupten Daten √ºberschrieben werden. Erst rename wenn vollst√§ndig + validiert.

**Wo**: `host/engine/shifter.py` ‚Üí `backup_tiktok_dual()`, `backup()` ‚Äî Atomic Write + Retry-Wrapper.

---

### ENTSCHEIDUNG Block 14.1 ‚Äî String-Verschl√ºsselung: SEPARATE PHASE
**Status**: Dokumentiert als eigene Phase NACH allen funktionalen Fixes.
**Begr√ºndung**: TikTok scannt nicht aktiv Zygisk-Module-Binaries. Die funktionalen Fixes (TikTok-Erkennung, Backup-L√ºcken, Auditor) haben h√∂here Priorit√§t. String-Verschl√ºsselung wird als Phase 7 eingeplant.

### FIX-24: String-Verschl√ºsselung + Raw Syscalls + memfd_create (Stealth-Hardening) ‚úÖ IMPLEMENTIERT
**Priorit√§t**: MITTEL ‚Äî Eigene Phase nach allen Flow-Fixes
**Problem**: Alle sensitiven Strings in `zygisk_module.cpp` und `titan_hardware.cpp` sind Klartext. `strings libtitan_zygisk.so` enth√ºllt das komplette Modul. Zus√§tzlich nutzt das Modul libc-Wrapper statt raw syscalls. Und es werden Temp-Dateien in `/data/local/tmp/` erstellt die nie gel√∂scht werden.

**Umfang (3 Teilbereiche)**:

**A) XOR-Verschl√ºsselung** f√ºr alle Strings (Pfade, Package-Namen, Log-Tags, Defaults):
```cpp
// Compile-Time XOR Macro
#define XOR_KEY 0x5A
#define DECRYPT(enc, len) ({ char* d = (char*)alloca(len+1); \
    for(int i=0;i<len;i++) d[i]=enc[i]^XOR_KEY; d[len]=0; d; })
```

**B) Raw Syscalls** (`syscall(__NR_openat, ...)`) statt `open()`, `read()`, `close()`, `stat()`:
```cpp
// Statt: int fd = open(path, O_RDONLY);
// Besser:
int fd = syscall(__NR_openat, AT_FDCWD, path, O_RDONLY, 0);
```

**C) `memfd_create` statt Temp-Dateien** (unauffindbarste L√∂sung):
```cpp
// Statt: open("/data/local/tmp/.titan_cpuinfo_1234", O_CREAT|O_RDWR, 0600)
// Besser:
int fd = syscall(__NR_memfd_create, "", MFD_CLOEXEC);
write(fd, fake_content, content_len);
lseek(fd, 0, SEEK_SET);
// ‚Üí Kein Dateisystem-Eintrag, nur anonymer RAM-FD
// ‚Üí find / -name '.titan*' findet NICHTS
// ‚Üí Existiert nur solange der Prozess lebt
// ‚Üí Funktioniert auf Android 14 (Kernel 5.10+, Pixel 6)
```

Betrifft folgende Temp-Dateien die aktuell erstellt werden:
- `/data/local/tmp/.titan_mac_open_<pid>` ‚Üí memfd_create
- `/data/local/tmp/.titan_input_open_<pid>` ‚Üí memfd_create
- `/data/local/tmp/.titan_cpuinfo_<pid>` ‚Üí memfd_create
- `/data/local/tmp/.titan_version_<pid>` ‚Üí memfd_create
- `/data/local/tmp/.titan_if_inet6_<pid>` ‚Üí memfd_create

Host-seitige Staging-Dateien (nach Push l√∂schen):
- `/data/local/tmp/.titan_bridge_staging` ‚Üí `adb shell rm` nach Push
- `/data/local/tmp/.titan_pif_staging.prop` ‚Üí `adb shell rm` nach Push

**Gesch√§tzter Aufwand**: 2-3 Tage (eigene Phase)

**Wo**: `module/zygisk_module.cpp`, `common/titan_hardware.cpp`, `common/titan_hardware.h`, `host/engine/injector.py` (Staging-Cleanup)

---

## SPARRING BLOCK 15 ‚Äî Logging & Observability

### FIX-25: Persistenter File-Logger mit Rotation ‚úÖ IMPLEMENTIERT
**Priorit√§t**: MITTEL
**Problem**: Logs existieren nur in-memory (Ring-Buffer, 500 Eintr√§ge) und im Terminal. Wenn der Server crasht oder neustartet, sind alle Logs weg. Post-Mortem-Analyse ist unm√∂glich.

**L√∂sung**: `RotatingFileHandler` in `host/main.py` hinzuf√ºgen:
```python
from logging.handlers import RotatingFileHandler

file_handler = RotatingFileHandler(
    "titan.log",
    maxBytes=10_000_000,    # 10 MB pro Datei
    backupCount=3,           # 3 alte Dateien behalten
    encoding="utf-8",
)
file_handler.setFormatter(_BerlinFormatter(...))
file_handler.setLevel(logging.DEBUG)  # Alles loggen, auch DEBUG
logging.root.addHandler(file_handler)
```

**Vorteile**:
- Max ~40MB Disk (10MB √ó 4 Dateien)
- DEBUG-Level im File (WebSocket bleibt auf INFO)
- Post-Mortem bei 3-Uhr-Nachts-Crashes m√∂glich
- ~10 Zeilen Code

**Wo**: `host/main.py` ‚Üí nach dem Console-Handler.

---

## SPARRING BLOCK 17 ‚Äî Frontend Konsistenz

### FIX-27: Unbenutzte Backend-Endpoints bereinigen ‚úÖ IMPLEMENTIERT
**Priorit√§t**: NIEDRIG
**Problem**: 7 Backend-Endpoints werden im Frontend nicht aufgerufen. Toter Code erh√∂ht Wartungsaufwand.

**Aktion ‚Äî Selektives Aufr√§umen**:

**BEHALTEN** (werden f√ºr Backup-Features gebraucht):
- `POST /api/control/backup` ‚Äî Backup-Flow manuell triggern
- `GET /api/vault/{id}/backups` ‚Äî Backup-Liste f√ºr ein Profil
- `POST /api/vault/{id}/backup` ‚Äî Manuelles Backup f√ºr ein Profil

**L√ñSCHEN** (redundant oder √ºber andere Wege erreichbar):
- `GET /api/dashboard/profiles` ‚Äî redundant mit `/api/vault`
- `GET /api/dashboard/farm-stats` ‚Äî redundant mit `/api/dashboard/stats`
- `PUT /api/vault/{id}/credentials` ‚Äî bereits √ºber `PUT /api/vault/{id}` (Edit) abgedeckt
- `PUT /api/vault/{id}/status` ‚Äî bereits √ºber Edit oder Bulk-Status abgedeckt

**Wo**: `host/api/dashboard.py` (profiles, farm-stats), `host/api/vault.py` (credentials, status)

---

### FIX-26: Polling-Guard gegen Race-Conditions ‚úÖ IMPLEMENTIERT
**Priorit√§t**: NIEDRIG
**Problem**: `pollFlowStatus()` im Dashboard wird alle 2s aufgerufen. Wenn ein API-Request l√§nger als 2s dauert, starten parallele Polls ‚Üí Doppel-Updates, UI-Flackern.

**L√∂sung**:
```javascript
let pollInProgress = false;
async function pollFlowStatus() {
    if (pollInProgress) return;
    pollInProgress = true;
    try {
        // ... bestehende Logik
    } finally {
        pollInProgress = false;
    }
}
```

Gleiches Muster f√ºr `refreshHeaderStatus()` und `pollFlowForHeader()` in `vault.html`.

**Wo**: `host/frontend/templates/dashboard.html` + `host/frontend/templates/vault.html`

---

## SPARRING BLOCK 9 ‚Äî Injector / Distribution

### FIX-19: Bridge-Distribution an Instagram + Snapchat (Vorbereitung) ‚úÖ IMPLEMENTIERT
**Priorit√§t**: NIEDRIG (erst relevant wenn Insta/Snap aktiv genutzt werden)
**Problem**: Die `BRIDGE_TARGET_APPS` in `host/config.py` enth√§lt TikTok, GMS, Titan Verifier, DRM Info und Device ID ‚Äî aber NICHT Instagram (`com.instagram.android`) und Snapchat (`com.snapchat.android`). 

Gleichzeitig hooked der **Zygisk-Module** (C++) und das **LSPosed-Module** (Kotlin) BEIDE Apps bereits aktiv. Wenn die Bridge-Datei fehlt, fallen die Hooks auf **hardcoded Default-Werte** zur√ºck (Zeile 83-89 in `zygisk_module.cpp`):
```cpp
static const char* DEFAULT_SERIAL = "28161FDF6006P8";
static const char* DEFAULT_IMEI1 = "352269111271008";
// ... etc.
```

Das bedeutet: Wenn Insta/Snap installiert und ge√∂ffnet werden, sehen sie ALLE die gleichen statischen Default-IDs ‚Üí sofortige Cross-App Correlation m√∂glich.

**L√∂sung (wenn Insta/Snap aktiviert werden)**:
1. `BRIDGE_TARGET_APPS` in `host/config.py` um `com.instagram.android` und `com.snapchat.android` erweitern
2. Der Injector verteilt die Bridge dann automatisch in die App-Ordner

**Status**: ‚úÖ Implementiert. `SOCIAL_MEDIA_PACKAGES` enth√§lt jetzt TikTok + Instagram + Snapchat. Bridge-Distribution pr√ºft via `test -d` ob die App installiert ist ‚Äî nicht installierte Apps werden √ºbersprungen.

**Wo**: `host/config.py` ‚Üí `BRIDGE_TARGET_APPS` via `SOCIAL_MEDIA_PACKAGES` erweitert.

---

### ERGEBNIS Block 10 ‚Äî GMS-Ausschluss in Zygisk: ‚úÖ BEREITS KORREKT
**Analyse**: Der Zygisk-Module hat eine explizite `TARGET_APPS[]` Whitelist (Zeile 71-80 in `zygisk_module.cpp`). GMS/GSF/Vending sind **bewusst ausgeschlossen** (Kommentar Zeile 66-70). In `preAppSpecialize()` (Zeile 1923) wird gepr√ºft: Wenn das Package NICHT in `TARGET_APPS` steht ‚Üí Modul wird mit `DLCLOSE_MODULE_LIBRARY` entladen. **Play Integrity ist sicher.**

---

### FIX-20: Hardcoded Default-Werte im Zygisk-Module entfernen ‚úÖ IMPLEMENTIERT
**Priorit√§t**: MITTEL
**Problem**: `zygisk_module.cpp` Zeilen 83-89 definieren statische Default-Werte:
```cpp
static const char* DEFAULT_SERIAL = "28161FDF6006P8";
static const char* DEFAULT_IMEI1 = "352269111271008";
static const char* DEFAULT_IMEI2 = "358476312016587";
static const char* DEFAULT_ANDROID_ID = "d7f4b30e1b210a83";
static const char* DEFAULT_GSF_ID = "3a8c4f72d91e50b6";
static const char* DEFAULT_WIFI_MAC = "be:08:6e:16:a6:5d";
static const char* DEFAULT_WIDEVINE_ID = "10179c6bcba352dbd5ce5c88fec8e098";
```

Diese werden als Fallback verwendet wenn die Bridge-Datei nicht geladen werden kann. Das Risiko: Wenn die Bridge aus irgendeinem Grund fehlt (Datei gel√∂scht, Permissions falsch, Race-Condition beim Boot), bekommen ALLE Target-Apps **dieselben statischen IDs**. Das ist schlimmer als keine Hooks ‚Äî weil es ein eindeutiger Fingerprint ist den kein echtes Ger√§t hat.

**Empfohlene √Ñnderung**: Statt statische Defaults ‚Üí **Hooks deaktivieren** wenn Bridge nicht geladen werden kann:
```cpp
// Statt:
if (!loadBridge()) { useDefaults(); }

// Besser:
if (!loadBridge()) {
    LOGW("[TITAN] Bridge nicht geladen ‚Äî Hooks DEAKTIVIERT");
    m_shouldInject = false;  // Keine Hooks ‚Üí echte Werte durchlassen
    return;
}
```

**Warum besser**: Echte Werte durchlassen ist weniger verd√§chtig als falsche statische Werte. Und der Auditor (FIX-17) w√ºrde den Fehler sofort erkennen.

**Wo**: `module/zygisk_module.cpp` ‚Üí Zeilen 83-89 (Defaults) + `loadBridge()` / `postAppSpecialize()` Fehlerbehandlung.

---

## SPARRING BLOCK 12 ‚Äî Datenbank-Konsistenz

### FIX-21: Foreign Key von RESTRICT auf CASCADE √§ndern ‚úÖ IMPLEMENTIERT
**Priorit√§t**: MITTEL
**Problem**: `profiles.identity_id` hat `ON DELETE RESTRICT`. Wenn eine Identit√§t gel√∂scht wird die noch ein Profil hat, schl√§gt der DELETE fehl mit `FOREIGN KEY constraint failed`.

**Gew√ºnschtes Verhalten** (User-Entscheidung): Identit√§t l√∂schen ‚Üí verlinktes Profil wird **automatisch mitgel√∂scht**.

**√Ñnderung**:
```sql
-- ALT:
identity_id INTEGER NOT NULL REFERENCES identities(id) ON DELETE RESTRICT

-- NEU:
identity_id INTEGER NOT NULL REFERENCES identities(id) ON DELETE CASCADE
```

**Achtung**: SQLite unterst√ºtzt kein `ALTER TABLE ... ALTER COLUMN`. Die √Ñnderung erfordert eine **Schema-Migration**:
1. Neue Tabelle `profiles_new` mit `ON DELETE CASCADE` erstellen
2. Daten von `profiles` ‚Üí `profiles_new` kopieren
3. Alte Tabelle l√∂schen
4. Neue Tabelle umbenennen

**Wo**: `host/database.py` ‚Üí Schema `_SQL_CREATE_PROFILES` + Migration-Logik in `_run_migrations()`.

---

## SPARRING BLOCK 9 ‚Äî Flow-Robustheit (Genesis + Switch)

### FIX-28: Genesis ‚Äî Sichere App-Reinstallation (FIX-13 Fallback-Bug) ‚úÖ IMPLEMENTIERT
**Priorit√§t**: KRITISCH
**Problem**: FIX-13 (`pm uninstall --user 0` + `pm install-existing`) hat einen kritischen Bug in der Fallback-Kette:
1. `pm uninstall --user 0 <pkg>` wird ausgef√ºhrt ‚Üí App f√ºr User 0 entfernt
2. `pm install-existing --user 0 <pkg>` schl√§gt fehl (Pixel 6 / Android 14 ‚Äî TikTok ist kein System-Package, Cache nicht verf√ºgbar)
3. Fallback: `pm clear <pkg>` ‚Üí tut **nichts**, weil die App bereits deinstalliert ist
4. Ergebnis: TikTok ist nach dem Hard Reset in Step 7 **verschwunden**

**L√∂sung**: APK-Pfad vor Uninstall sichern + mehrstufige Rettungskette:
1. **Schritt 0**: `pm path <pkg>` ‚Üí APK-Pfad sichern (z.B. `/data/app/~~abc/base.apk`)
2. **Schritt 1**: `pm uninstall --user 0 <pkg>`
3. **Schritt 2**: `pm install-existing --user 0 <pkg>` versuchen
4. **Schritt 3**: `pm path <pkg>` ‚Üí Verifikation ob App noch da ist
5. **Schritt 4** (Rettung): `pm install -r --user 0 <gespeicherter_pfad>` wenn App weg
6. **Schritt 5** (Letzter Fallback): `cmd package install-existing <pkg>`
7. **Sicherer Modus**: Wenn APK-Pfad nicht ermittelbar ‚Üí kein `pm uninstall`, nur `pm clear`

**Neue Methoden in `shifter.py`**:
- `_get_apk_path(pkg)` ‚Üí Ermittelt APK-Pfad via `pm path`
- `_verify_app_installed(pkg)` ‚Üí Pr√ºft ob App f√ºr User 0 verf√ºgbar ist

**Wo**: `host/engine/shifter.py` ‚Üí `deep_clean()` Schritt 1 (komplett √ºberarbeitet)

---

### FIX-29: Switch ‚Äî Gr√ºndlicher State-Wipe vor Restore ‚úÖ IMPLEMENTIERT
**Priorit√§t**: HOCH
**Problem**: Zwei Schwachstellen im Switch Flow:

**A) Kein Clean vor Restore:**
Zwischen Safety Kill (Step 3) und Restore (Step 5/6) sitzen die alten App-Daten unangetastet. Der Mini-Clean (FIX-16) bereinigt nur `/sdcard/` Tracking-Dateien, nicht die App-Daten in `/data/data/`. Wenn der Restore nur teilweise √ºberschreibt, leaken alte Profil-Daten durch.

**B) Hidden-File-Bug in Restore-Methoden:**
`restore()` und `restore_tiktok_dual()` verwenden `rm -rf <path>/*` ‚Äî der `/*`-Glob verpasst **dot-files** (z.B. `.device_id`, `.tt_session`, `.tobid_v2`). TikTok legt bewusst versteckte Tracking-Dateien an, die so den Restore √ºberleben.

**L√∂sung**:

**Teil 1 ‚Äî Neue `prepare_switch_clean()` Methode** (ersetzt FIX-16 Mini-Clean):
- L√∂scht `/data/data/<pkg>/` **komplett** (nicht nur `/*`) + neu erstellen
- L√∂scht TikTok Sandbox-Verzeichnisse auf `/sdcard/`
- Bereinigt alle Tracking-Globs + ByteDance Deep-Search Patterns
- L√∂scht ART Compiler Cache + Runtime Profiles
- Bereinigt Settings-ContentProvider (FIX-14 Werte)

**Teil 2 ‚Äî Hidden-File-Bug Fix in `restore()` + `restore_tiktok_dual()`:**
```python
# ALT (verpasst dot-files):
rm -rf /data/data/<pkg>/*

# NEU (l√∂scht ALLES):
rm -rf /data/data/<pkg>
mkdir -p /data/data/<pkg>
```

**Teil 3 ‚Äî Switch Flow Step 3 erweitert:**
`clean_tracking_remnants()` (FIX-16) durch `prepare_switch_clean()` (FIX-29) ersetzt.

**Wo**:
- `host/engine/shifter.py` ‚Üí Neue Methode `prepare_switch_clean()`, `restore()` rm-Fix, `restore_tiktok_dual()` rm-Fix
- `host/flows/switch.py` ‚Üí Step 3‚Üí4 Transition: `prepare_switch_clean()` statt `clean_tracking_remnants()`

---

### FIX-30: Switch ‚Äî Post-Restore Verifikation + Zombie-Schutz ‚úÖ IMPLEMENTIERT
**Priorit√§t**: HOCH
**Problem**: Nach dem Restore gibt es keine Pr√ºfung ob die App-Daten tats√§chlich geschrieben wurden. Wenn der Restore stillschweigend fehlschl√§gt:
- Bridge zeigt auf Identity B (neu)
- App-Daten sind leer oder geh√∂ren zu Identity A (alt)
- TikTok startet als "neue App" mit der falschen Identit√§t ‚Üí Detection

**L√∂sung**:

**Neue `verify_app_data_restored(pkg)` Methode in `shifter.py`:**
Pr√ºft drei Kriterien:
1. `/data/data/<pkg>/` existiert
2. Mindestens `shared_prefs/`, `databases/` oder `files/` existiert
3. Verzeichnis hat > 0 Eintr√§ge (nicht leer)

Gibt detailliertes Dict zur√ºck: `{ok, dir_exists, has_prefs, has_databases, has_files, file_count, detail}`

**Post-Restore Check im Switch Flow (zwischen Step 6 und 7):**
- Wird nach TikTok Restore ausgef√ºhrt (nur bei Full-State oder profile_name Modus)
- Bei Fehlschlag: **Zombie-Schutz** ‚Äî `pm clear <pkg>` um inkonsistenten State zu verhindern
- Step 6 wird nachtr√§glich als FAILED markiert
- Klares Error-Logging mit Verifikations-Details

**Wo**:
- `host/engine/shifter.py` ‚Üí Neue Methode `verify_app_data_restored()`
- `host/flows/switch.py` ‚Üí Neuer Zwischenschritt 6‚Üí7: Post-Restore Verifikation

---

## BEREITS GEFIXTE FINDINGS

### FIX-8: Genesis Flow meldet FEHLGESCHLAGEN bei SKIPPED Steps
**Status**: ‚úÖ BEREITS GEFIXT (in dieser Session)
**Was**: `all(s.status == FlowStepStatus.SUCCESS ...)` z√§hlte SKIPPED als Failure.
**Fix**: `s.status in (FlowStepStatus.SUCCESS, FlowStepStatus.SKIPPED)`

---

## UMSETZUNGSREIHENFOLGE

### Phase 1: TikTok Fresh-Install Fix (KRITISCH)
1. **FIX-1 + FIX-2** ‚Äî ByteDance Deep-Search + Cache-Cleanup ‚Üí behebt TikTok-Wiedererkennung
2. **FIX-13** ‚Äî `pm uninstall --user 0` + `pm install-existing` ‚Üí echter First-Launch-State

### Phase 2: Flow-Stabilit√§t + Switch-Integrit√§t (HOCH)
3. **FIX-15** ‚Äî Sandbox-L√ºcke im Switch Flow ‚Üí TikTok Sandbox-Restore reparieren
4. **FIX-16** ‚Äî Mini-Clean vor Switch-Restore ‚Üí ByteDance-Tracking-Reste entfernen
5. **FIX-10** ‚Äî GMS Ready vereinfachen ‚Üí eliminiert Flow-H√§nger
6. **FIX-11** ‚Äî TikTok Backup-Logik ‚Üí intelligentes Auto-Backup
7. **FIX-3** ‚Äî Backup-Whitelist ‚Üí sauberere Backups

### Phase 3: Verifikation & IP-Sicherheit (HOCH)
8. **FIX-17** ‚Äî Auditor Full + Quick Audit erweitern ‚Üí alle Spoofing-Felder pr√ºfen
9. **FIX-18** ‚Äî IP-Collision-Detection ‚Üí Cross-Profile IP-Korrelation erkennen

### Phase 4: Error Handling & Resilience (HOCH)
10. **FIX-22** ‚Äî Genesis Rollback erweitern ‚Üí `corrupted` nach Inject bei Fehler + WebUI-Info
11. **FIX-23** ‚Äî Backup Atomic Write + Retry ‚Üí keine korrupten Backups + WebUI-Info bei Abbruch

### Phase 5: Robustheit (HOCH ‚Üí MITTEL)
12. **FIX-7** ‚Äî `wm dismiss-keyguard` ‚Üí 3 Zeilen, sofort wirksam
13. **FIX-5** ‚Äî CE-Storage Check ‚Üí robusterer Unlock-Check
14. **FIX-6** ‚Äî USB-Reconnect ‚Üí ADB-Zombie-State Fallback

### Phase 6: Verifikation & Monitoring (MITTEL)
15. **FIX-9** ‚Äî Bridge-Verifikation alle Pfade ‚Üí vollst√§ndige Post-Reboot-Pr√ºfung
16. **FIX-4** ‚Äî Integrity Guard ‚Üí Backup-Validierung (braucht Testing)
17. **FIX-14** ‚Äî Settings-ContentProvider Cleanup ‚Üí TikTok System-Settings bereinigen
18. **FIX-12** ‚Äî Xposed Debug-Log-Mode ‚Üí Hook-Monitoring in WebUI
19. **FIX-20** ‚Äî Hardcoded Defaults im Zygisk entfernen ‚Üí Bridge-Fehler = Hooks aus
20. **FIX-21** ‚Äî ON DELETE CASCADE ‚Üí Profil wird mit Identit√§t mitgel√∂scht
21. **FIX-25** ‚Äî Persistenter File-Logger mit Rotation ‚Üí Post-Mortem m√∂glich
22. **FIX-26** ‚Äî Polling-Guard ‚Üí keine Race-Conditions im Frontend
23. **FIX-27** ‚Äî Unbenutzte Endpoints l√∂schen (4 von 7), Backup-Endpoints behalten

### Phase 7: Stealth-Hardening (MITTEL ‚Äî eigene Phase) ‚úÖ ABGESCHLOSSEN
24. **FIX-24** ‚Äî String-Verschl√ºsselung + Raw Syscalls + memfd_create ‚úÖ

### Phase 8: Vorbereitung Multi-App (NIEDRIG ‚Äî erst wenn Insta/Snap aktiviert) ‚úÖ ABGESCHLOSSEN
25. **FIX-19** ‚Äî Bridge-Distribution an Instagram + Snapchat ‚úÖ

### Phase 9: Flow-Robustheit ‚Äî Genesis + Switch (KRITISCH) ‚úÖ ABGESCHLOSSEN
26. **FIX-28** ‚Äî Genesis: Sichere App-Reinstallation (APK-Pfad sichern + Verifikation) ‚úÖ
27. **FIX-29** ‚Äî Switch: Gr√ºndlicher State-Wipe vor Restore (Hidden-File-Bug + prepare_switch_clean) ‚úÖ
28. **FIX-30** ‚Äî Switch: Post-Restore Verifikation + Zombie-Schutz ‚úÖ

---

## DATEIEN DIE GE√ÑNDERT WERDEN

| Datei | Fixes |
|-------|-------|
| `host/engine/shifter.py` | FIX-1, FIX-2, FIX-3, FIX-4, FIX-5, FIX-13, FIX-14, FIX-16, FIX-28, FIX-29, FIX-30 |
| `host/adb/client.py` | FIX-6, FIX-7 |
| `host/flows/genesis.py` | FIX-9, FIX-10, FIX-11, FIX-18 |
| `host/flows/switch.py` | FIX-11, FIX-15, FIX-16, FIX-18, FIX-29, FIX-30 |
| `host/engine/auditor.py` | FIX-17 |
| `host/engine/db_ops.py` | FIX-18 |
| `host/database.py` | FIX-18 (Indizes aktivieren) |
| `app/.../TitanXposedModule.kt` | FIX-12 |
| `host/models/identity.py` | FIX-12 (Bridge-Feld `debug_hooks`) |
| `host/frontend/templates/dashboard.html` | FIX-18 (optional: IP-Metriken), FIX-22 + FIX-23 (WebUI-Meldungen) |
| `host/config.py` | FIX-19 (BRIDGE_TARGET_APPS erweitern) |
| `module/zygisk_module.cpp` | FIX-20 (Defaults entfernen), FIX-24 (XOR + Syscalls) |
| `common/hw_compat.cpp` (ehem. titan_hardware.cpp) | FIX-24 (XOR + Syscalls + memfd_create), FIX-31 (Stealth-Rename) |
| `host/main.py` | FIX-25 (File-Logger) |
| `host/frontend/templates/vault.html` | FIX-26 (Polling-Guard) |
| `host/api/vault.py` | FIX-27 (Endpoints l√∂schen: credentials, status) |
| `host/api/dashboard.py` | FIX-27 (Endpoints l√∂schen: profiles, farm-stats) |

---

## REFERENZ-PROJEKT

Ares/Maschina Pfad: `/Users/arvin/Documents/Android/Chaos/Android Automatisierung Maschina/`
Relevante Dateien:
- `core/shifter.py` ‚Äî Deep Sanitize, Dual-Path Backup, Permissions, Integrity Guard
- `core/generator.py` ‚Äî Identity Generator (√§hnlich wie Titan, aber JSON-basiert)
- `core/injector.py` ‚Äî Android Faker Injection (anderer Ansatz)
- `config.py` ‚Äî ADB/Unlock-Konfiguration (Referenz f√ºr Timing-Werte)

---

## FIX-31: Operation Tarnkappe ‚Äî Stealth-Hardening (Komplett-Rename)

**Problem:** Alle identifizierbaren Strings ("titan", "verifier", Package-Name, Log-Tags, Klassen-
namen, Dateipfade) waren im Klartext im Code und Binary. Anti-Cheat-Engines k√∂nnten diese via
`strings`, `logcat`, `ls /data/adb/modules/`, oder Package-Scanning erkennen.

**L√∂sung:** Umfassendes Renaming √ºber das GESAMTE Projekt:

| Kategorie | Alt | Neu |
|-----------|-----|-----|
| Package-Name | `com.titan.verifier` | `com.oem.hardware.service` |
| Modul-ID | `titan_verifier` | `hw_overlay` |
| Bridge-Datei | `.titan_identity` | `.hw_config` |
| Kill-Switch | `titan_stop` | `.hw_disabled` |
| SO-Datei | `libtitan_zygisk.so` | `libhw_overlay.so` |
| App-Label | `Titan Verifier` | `Hardware Service` |
| Log-Tags | `TitanZygisk` / `TitanBridge` | DEAKTIVIERT (STEALTH_MODE) |
| C++ Klassen | `TitanModule` / `TitanHardware` | `CompatModule` / `HwCompat` |
| Kotlin Klassen | `TitanXposedModule` / `TitanBridgeReader` | `TelephonyServiceModule` / `ServiceConfigReader` |
| Python Klassen | `TitanShifter` / `TitanAuditor` / `TitanInjector` | `AppShifter` / `DeviceAuditor` / `BridgeInjector` |
| Logger | `titan.*` | `host.*` |
| API-Titel | `Project Titan ‚Äî Command Center` | `Device Manager` |
| Datenbank | `titan.db` | `device_manager.db` |
| Log-Datei | `titan.log` | `host.log` |

**Betroffene Dateien:** 40+ (C++, Kotlin, Python, XML, HTML, Gradle, CMake)

**Verifikation:** `grep -ri "titan" --include="*.{py,cpp,h,kt,kts,xml,pro,html}" .` ‚Üí **0 Treffer**
(Nur Build-Cache in `.cxx/` enth√§lt noch alte Referenzen ‚Äî wird beim n√§chsten Build regeneriert)

**WICHTIG ‚Äî Device-Deployment erforderlich:**
1. Altes Modul deinstallieren: `adb shell rm -rf /data/adb/modules/titan_verifier`
2. Alte App deinstallieren: `adb uninstall com.titan.verifier`
3. Alte Bridge-Dateien entfernen: `adb shell rm /sdcard/.titan_identity /data/local/tmp/titan_stop`
4. Neues Modul deployen (hw_overlay) + neue App installieren (com.oem.hardware.service)
5. `.cxx/` Build-Cache l√∂schen und Android-App neu bauen

Siehe: **STEALTH_PLAN.md** f√ºr vollst√§ndige Details.
